{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### □ 사전준비 □"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Google 검색출력화면의 이해필요\n",
    "\n",
    "2) 검색결과창에서 F12로 구성내역 확인 → 각 요소의 attribute 명칭, class 명칭 파악\n",
    "\n",
    "※ 주의 - 과거이력을 볼 때, 구글 맘대로 요소 attribute 내역이나 class 명칭을 바꾸거나 파서가 바뀔 수도 있는 것 같음 (지꺼니까 당연한가;;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ■ 개괄설명 ■"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) '검색어(Keywords)' 및 '조회페이지 수' 임의입력\n",
    "\n",
    "2) selenium 라이브러리 內 webdriver 활용한 브라우저(크롬) 제어\n",
    "\n",
    "3) page_source확보후 Beautifulsoup로 파싱(parcing)\n",
    "\n",
    "4) 사전 살펴본  attribute 명칭, class 명칭으로 원하는 요소 데이터를 찾음\n",
    "\n",
    "5) 보고싶은 자료 형태로 가공"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ■ 코딩내역 ■ (잡설포함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 설치 명령어\n",
    "! pip install requests\n",
    "! pip install selenium\n",
    "! pip install bs4\n",
    "! pip install openpyxl\n",
    "! pip install pandas\n",
    "! pip install numpy\n",
    "! pip install re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 관련 라이브러리 목록 \"\"\"\n",
    "#-*- coding:utf-8 -*-  # 한글 안깨질려고 utf-8 인코딩\n",
    "# Crawling 관련 라이브러리\n",
    "import urllib.request\n",
    "from urllib import parse\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "\n",
    "# 데이터 핸들링 관련 라이브러리\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "\"\"\" 검색어(≒키워드) 관리 \"\"\"\n",
    "keywords = ['페코린느'] # , '고구마' , '호박']\n",
    "\n",
    "\"\"\" 조회 페이지 조정 \"\"\"\n",
    "number_of_pages = 2\n",
    "\n",
    "\"\"\" 결과삽입용 데이터 프레임 미리 만들어 놓음(여기에 keywords에 따른 결과를 하나하나 넣을 것임) \"\"\"\n",
    "result= pd.DataFrame([])\n",
    "\n",
    "\"\"\" 크롤링 \"\"\"\n",
    "# 크롤링은 kewords 리스트에 있는 단어들을 하나하나 수행하는게 필요. \n",
    "# kewords 리스트에 '페코린느' 하나만 있으면 한 번 수행하겠지만? 여러개면 kewords 리스트 안에껄 전부완료 할 때까지 반복작업해야함\n",
    "# 또한 kewords 별로, 구글 검색결과 '몇' 페이지까지 조회하는지 설정두 해야함. 그러니까 단어 하나하나, 설정한 조회페이지수 마지막까지 들여다 보게해야함\n",
    "# 그래서 반복작업이 필요하니, for 문으로 loop를 돌리는 거고,\n",
    "# 단어수 * 조회페이지수 두 가지 기준으로 반복되니, for문을 두개 겹쳐 쓴 것\n",
    "\n",
    "for keyword in keywords: # keywords 리스트 안에서, keyword 하나 선택\n",
    "    for number in range(0, number_of_pages): # number_of_pages 결정하면 페이지 수대로 검색\n",
    "        \n",
    "        # keyword 문자 그대로 url에 인풋\n",
    "        url = 'https://www.google.com/search?q={}&start={}'.format(keyword , int(number*10)) # 예 : keywords = '프리코네'\n",
    "\n",
    "        # keyword 문자를 url 인코딩해서 인풋 \n",
    "        # 혹시나 나중에라도,사이트+검색어 부분에서 검색어세 인코딩된 내역을 요구할까봐 남겨둠\n",
    "        # keyword_encoded = parse.quote(keyword)\n",
    "        # url = 'https://www.google.com/search?q={}&start={}'.format(keyword , int(number*10)) # 예 : keywords = '%ED%94%84%EB%A6%AC%EC%BD%94%EB%84%A4'\n",
    "\n",
    "        ######################################################################################\n",
    "        ######################################################################################\n",
    "        \"\"\" 주의 : 하기 Chorm() 괄호에는 반드시 자신 컴퓨터의 chrome 깔린 위치를 입력해줘야함 \"\"\"\n",
    "        # webdriver 란 selenium 클래스한테, Chrome 사용지시\n",
    "        driver = webdriver.Chrome('C:/Users/amore/Downloads/chromedriver_win32/chromedriver')  # 반드시 자신 컴퓨터의 크롬 깔린 폴더경로 입력\n",
    "        ######################################################################################\n",
    "        ######################################################################################\n",
    "\n",
    "        # 크롬먹은 webdriver(이하 driver)에게 url 꽂아주면서 켜라고 지시 (사이트+검색어 내역을 꽂음)\n",
    "        driver.get(url)\n",
    "\n",
    "        # driver 한테서 page_source를 뱉어내게하고, \n",
    "        # BeatifulSoup 보고 html 코드니까 해석하라고 시킴\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "        # 구글 검색페이지 2020-03-31 기준,\n",
    "        # 사이트 이름의 경우, attribute 이름 h3 로\n",
    "        # 광고일 경우 class 가 sA5rQ , 일반페이지일경우 class 가 LC20lb DKV0Md\n",
    "        # 사이트 주소의 경우, arrtibute 이름 div로\n",
    "        # 광고일 경우 class 가 ad_cclk , 일반페이지일경우 class 가 r\n",
    "        # 이거 추후에 바뀔 수도 있으므로 주의\n",
    "        total_names = soup.find_all('h3', {\"class\": [\"sA5rQ\", \"LC20lb DKV0Md\"]})\n",
    "        total_links = soup.find_all('div', {\"class\": [\"ad_cclk\", \"r\"]})\n",
    "\n",
    "        # 음..이하는 그냥.. 노가다 작업.\n",
    "        # 구글이 코딩해놓은 사이트 규칙에 맞게 필요한 정보를 추출함\n",
    "        # 추출하는건 re 라이브러리 findall 함수를 썼음\n",
    "\n",
    "        # 광고페이지냐 일반페이지냐 솔팅\n",
    "        total_names_list = [''.join(re.findall(r'(<h3 class=\"LC20lb DKV0Md\">.*|<h3 class=\"sA5rQ\">.*)', item )) for item in str(total_names)[1:-1].split('</h3>, ') ]\n",
    "        Separator = [item.replace('<h3 class=\"LC20lb DKV0Md\">', '일반 ') for item in total_names_list]\n",
    "        Separator = [item.replace('<h3 class=\"sA5rQ\">', '광고 ') for item in Separator]\n",
    "        Separator = [ x for x in Separator if (\"일반 \" in x) | (\"광고 \" in x)]\n",
    "        Separator= [item[:2] for item in Separator]\n",
    "\n",
    "        # 사이트 이름 추출\n",
    "        total_names_list = [item.replace('<h3 class=\"LC20lb DKV0Md\">', '') for item in total_names_list]\n",
    "        total_names_list = [item.replace('<h3 class=\"sA5rQ\">', '') for item in total_names_list]\n",
    "        total_names_list = [item.replace('\\u200e', '') for item in total_names_list]\n",
    "        total_names_list = [item.replace('</h3>', '') for item in total_names_list]\n",
    "\n",
    "        # 사이트 링크 추출\n",
    "        # 흠.. 광고의 경우, 사이트를 두개를 가지고 있어서 ( 하나는 data-preconnect-url , 하나는 href) 혹시 몰라냄겨둠\n",
    "        # total_links_list = [''.join(re.findall(r'(data-preconnect-urls=.*\"|href=.*)', item )) for item in str(total_links).replace('https','http').split(' ')]\n",
    "        total_links_list = [''.join(re.findall(r'(href=.*)', item )) for item in str(total_links).replace('https','http').split(' ')]\n",
    "        total_links_list = [ x for x in total_links_list if \"webcache\" not in x ]\n",
    "        # total_links_list = [ x for x in total_links_list if 'href=\"/aclk?sa' not in x ]  # 얘는 그냥 기준으로 못쓰겠다.. \n",
    "        total_links_list = [ x for x in total_links_list if 'amp;adurl=\"' not in x ]\n",
    "        total_links_list = [ x for x in total_links_list if 'href=\"/search?q=related' not in x ]\n",
    "        total_links_list = list(filter(lambda a: (a != '')&(a != 'href=\"#\"'), total_links_list))\n",
    "\n",
    "        # href= 구분자로 물고 온지라, 앞에서 떼어냄, 앞뒤의 \"\" 도 삭제\n",
    "        total_links_list = [ item[6:-1] for item in total_links_list ]\n",
    "\n",
    "        # 임시표 temp 작성\n",
    "        temp = pd.DataFrame([])\n",
    "        temp['keywords'] = '' # Separator 나 사이트이름이나, 주소들은 모두 여러개인데, keywords는 하나니까 자리만 뚫어놓고\n",
    "        temp['구분'] = Separator\n",
    "        temp['노출순위\\n(광고포함)'] = ''\n",
    "        temp['노출순위\\n(광고제외)'] = ''\n",
    "        temp['표시페이지'] = number+1\n",
    "        temp['사이트명'] = total_names_list\n",
    "        temp['사이트주소'] = total_links_list\n",
    "        temp['노출도평가'] = ''\n",
    "        temp['keywords'] = keyword # 자리가 다 들어차면 keywords 이름 던져줌\n",
    "\n",
    "        # 미리 만들어논 result 에 temp 삽입\n",
    "        result = pd.concat([result, temp] , axis=0)\n",
    "\n",
    "        # temp 토사구팽\n",
    "        del temp\n",
    "        \n",
    "        # 크롤링 다했으면 프-바\n",
    "        driver.close()\n",
    "\n",
    "    \"\"\" 순위부여 \"\"\"\n",
    "    result['노출순위\\n(광고포함)'] = list(range(1 , result.shape[0]+1))\n",
    "    result['노출순위\\n(광고제외)'] = np.where( result['구분'] == '일반' , 1 , 0 )\n",
    "    result['노출순위\\n(광고제외)'] =result['노출순위\\n(광고제외)'].cumsum()\n",
    "    result['노출순위\\n(광고제외)'] = np.where( result['구분'] == '일반' ,result['노출순위\\n(광고제외)'] , '' )\n",
    "    \n",
    "    \"\"\" 노출도평가 \"\"\"\n",
    "    # 1 페이지 3위이내 -> 검색어 친 후, 유저 가시권해당 -> 최상\n",
    "    # 1 페이지 하위 3등 -> 검색어 친 후, 굳이 스크롤 내려야 보임 -> 중\n",
    "    # 1 페이지 중간층 -> 검색어 친 후, 스윽 하면 보임 -> 상\n",
    "    # 1 페이지 아님 -> 검색어 친 후, 굳이 수고를 들여 클릭해야함, \n",
    "    #                  즉 클릭하는 사용자는'뭔가'를 찾는 니즈를 이미 가지고 있음. 2페이지 노출된다고 중? 이건 아닌거같음 -> 하\n",
    "    result['노출순위\\n(광고포함)'] = result['노출순위\\n(광고포함)'].astype(int)\n",
    "    result['노출도평가'] = '하'\n",
    "    result['노출도평가'] = np.where( result['노출순위\\n(광고포함)'] <= 3, '최상', \n",
    "                           np.where( (result['노출순위\\n(광고포함)'] >= result[result['표시페이지'] == 1].shape[0]-2) & (result['표시페이지']==1), '중',\n",
    "                           np.where(  result['표시페이지'] != 1 , '하' , '상')))\n",
    "    \n",
    "\"\"\" 파일저장 \"\"\"\n",
    "\"\"\" 아래 to_excel() 요기에 엑셀이 저장될 위치를 넣어줘야함  \"\"\"\n",
    "result.to_excel('C:/Users/amore/Desktop/test.xlsx' , # ◀ ★(중요)★ excel 파일 저장할 위치 설정\n",
    "                header = True, index = False,  # 헤더는 ok , 인덱스는 노노해\n",
    "                startrow = 1, startcol = 1, # 1행 1열부터 데이터 푸셈\n",
    "                freeze_panes = (2, 0))  # 2행부터 행고정 거셈"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jarvis",
   "language": "python",
   "name": "jarvis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
